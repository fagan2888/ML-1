{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate starting time\n",
    "import time\n",
    "start_time = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "#load datasets\n",
    "def loadDataset (trainfile, testfile):\n",
    "    fileData = open(trainfile, \"r\",encoding='utf-8-sig')\n",
    "    lines = fileData.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        x = line.split()\n",
    "        data.append(x)\n",
    "    \n",
    "    df = pd.DataFrame(data,index = None, columns = list(range (len(data[0]))) )\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    col = len (df.columns)\n",
    "    trainingSet = df.iloc[:,0:col].values.tolist()\n",
    "    \n",
    "    # read the test data file\n",
    "    fileData1 = open(testfile, \"r\",encoding='utf-8-sig')\n",
    "    liness = fileData1.readlines()\n",
    "    \n",
    "    dataT = []\n",
    "    for line in liness:\n",
    "        x = line.split()\n",
    "        dataT.append(x)\n",
    "    \n",
    "    df1 = pd.DataFrame(dataT,index = None, columns = list(range (len(dataT[0]))) )\n",
    "    df1 = df1.apply(pd.to_numeric)\n",
    "    col1 = len (df1.columns)\n",
    "    testSet = df1.iloc[:,0:col1].values.tolist()\n",
    "    return trainingSet, testSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Euclidean distances \n",
    "def euclidean_distance(test_sample, train_set, length):\n",
    "    d = 0\n",
    "    for x in range(length):\n",
    "        d += np.square(test_sample[x] - train_set[x])\n",
    "    return np.sqrt(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select neighbors based on calculation\n",
    "def neighbors(training_set, test_sample, K):\n",
    "    d_set = []\n",
    "    length = len(test_sample)-1\n",
    "    for x in range(len(training_set)):\n",
    "        dist = euclidean_distance(test_sample, training_set[x], length)\n",
    "        d_set.append((training_set[x], dist))\n",
    "    d = sorted(d_set, key=itemgetter(1))\n",
    "    neighbor_set = []\n",
    "    for k in range(K):\n",
    "        neighbor_set.append(d[k][0])\n",
    "    return neighbor_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07744399999999985 seconds\n"
     ]
    }
   ],
   "source": [
    "#measure runtime after implementation\n",
    "print (time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification with variable priors\n",
    "def classification(neighbor,prior0,training_set):\n",
    "    n = len(training_set)\n",
    "    m = len(training_set[0]) - 1\n",
    "    n0 = len([x for x in training_set if x[:][m]==0])\n",
    "    n1 = len([x for x in training_set if x[:][m]==1])\n",
    "\n",
    "    k = len(neighbor)\n",
    "    l = len(neighbor[0]) - 1\n",
    "    k0 = len([x for x in neighbor if x[:][l]==0])\n",
    "    k1 = len([x for x in neighbor if x[:][l]==1])\n",
    "    prob0 = ((k0/k)*(prior0))/(n0/n)\n",
    "    prior1 = 1 - prior0\n",
    "    prob1 = ((k1/k)*prior1)/(n1/n)\n",
    "    if prob0 > prob1:\n",
    "        cls = 0\n",
    "    else:\n",
    "        cls = 1\n",
    "    return cls, prior0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10956099999999935 seconds\n"
     ]
    }
   ],
   "source": [
    "#measure runtime\n",
    "print (time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run KNN altogether for predictions\n",
    "def KNN(training_set,test_set,K,prior0):\n",
    "    prediction=[]\n",
    "    for x in range(len(test_set)):\n",
    "        neighbor_data = neighbors(training_set, test_set[x], K)\n",
    "        result,prior0 = classification(neighbor_data,prior0,training_set )\n",
    "        prediction.append(result)\n",
    "    return prior0,prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12535199999999946 seconds\n"
     ]
    }
   ],
   "source": [
    "#measure runtime\n",
    "print (time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate TP,TN,FP,FN values\n",
    "def performance_measure(test_set, predicted_class):\n",
    "    n = len(test_set[0])-1\n",
    "    true_class = []\n",
    "    for i in range(len(test_set)):\n",
    "        x = test_set[i][n]\n",
    "        true_class.append(x)   \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(predicted_class)): \n",
    "        if true_class[i]==predicted_class[i]==1:\n",
    "            TP += 1\n",
    "        if true_class[i]==predicted_class[i]==0:\n",
    "            TN += 1\n",
    "        if predicted_class[i]==1 and true_class[i]!=predicted_class[i]:\n",
    "            FP += 1\n",
    "        if predicted_class[i]==0 and true_class[i]!=predicted_class[i]:\n",
    "            FN += 1      \n",
    "    posneg = [TP, FP, TN, FN]\n",
    "    \n",
    "    return posneg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingSet, testSet = loadDataset ('nX_PIMA_TR.txt','nX_PIMA_TE.txt')\n",
    "prior0,predictions = KNN(trainingSet,testSet,15,132/200)\n",
    "performance_nX = performance_measure(testSet, predictions)\n",
    "\n",
    "trainingSetp, testSetp = loadDataset ('pX_PIMA_TR.txt','pX_PIMA_TE.txt')\n",
    "prior0p,predictionsp = KNN(trainingSetp,testSetp,11,132/200)\n",
    "performance_pX = performance_measure(testSetp, predictionsp)\n",
    "\n",
    "trainingSetf, testSetf = loadDataset ('fX_PIMA_TR.txt','fX_PIMA_TE.txt')\n",
    "prior0f,predictionsf = KNN(trainingSetf,testSetf,7,132/200)\n",
    "performance_fX = performance_measure(testSetf, predictionsf)\n",
    "\n",
    "knn = performance_nX\n",
    "knnP = performance_pX\n",
    "knnF = performance_fX\n",
    "\n",
    "# print (knn)\n",
    "# print (knnP)\n",
    "# print (knnF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy (the probability of a correct decision)\n",
    "def getAccuracy(performance_list):\n",
    "    acc = (performance_list[0]+performance_list[2])/sum(performance_list)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_knn_N = getAccuracy (knn)\n",
    "acc_knn_P = getAccuracy (knnP)\n",
    "acc_knn_F = getAccuracy (knnF)\n",
    "\n",
    "# print (acc_knn_N)\n",
    "# print (acc_knn_P)\n",
    "# print (acc_knn_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy for diffrent K\n",
    "def vary_k(trainingSet,testSet,prior0):\n",
    "    K = [x for x in range (1,16)]\n",
    "    accuracy = []\n",
    "    for k in K:\n",
    "        prior0,predictions = KNN(trainingSet,testSet,k,prior0)\n",
    "        performance = performance_measure(testSet, predictions)\n",
    "        acc = getAccuracy (performance)    \n",
    "        accuracy.append(acc)\n",
    "    return K, accuracy\n",
    "\n",
    "# vary_k (trainingSet,testSet,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sensitivity,specificity, TPR, FPR\n",
    "def performance(performance_list):\n",
    "    sensitivity = performance_list[0]/(performance_list[0]+performance_list[3])\n",
    "    specificity = performance_list[2]/(performance_list[2]+performance_list[1])\n",
    "    TPR = sensitivity\n",
    "    FPR = 1 - specificity\n",
    "    acc = [sensitivity,specificity, TPR, FPR]\n",
    "    return acc\n",
    "\n",
    "# print(performance(knn))\n",
    "# print(performance(knnP))\n",
    "# print(performance(knnF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sensitivity,specificity, TPR, FPR for different prior probabilities\n",
    "def perf_list(trainingSet,testSet,k):\n",
    "    prior0 = []\n",
    "    x = 0\n",
    "    while x<=1:\n",
    "        x+=.005\n",
    "        prior0.append(x)\n",
    "\n",
    "    perf = []\n",
    "    for p in prior0:\n",
    "        pri,predictions = KNN(trainingSet,testSet,k,p)\n",
    "        perform = performance_measure(testSet, predictions)\n",
    "        acc = performance(perform)\n",
    "        perf.append(acc)\n",
    "        \n",
    "    return perf\n",
    "\n",
    "# perf_list(trainingSet,testSet,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "#plot ROC curve and calculate the area under the curve\n",
    "def plot_roc(perf_list):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    lw = 2\n",
    "\n",
    "    TPR = []\n",
    "    FPR = []\n",
    "    for i in range(len(perf_list)):\n",
    "        x = perf_list[i][2]\n",
    "        TPR.append(x)\n",
    "        y = perf_list[i][3]\n",
    "        FPR.append(y)\n",
    "        \n",
    "    roc_auc = auc(FPR, TPR) \n",
    "\n",
    "    plt.plot(FPR, TPR, color='darkorange',lw=lw,label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "#     plt.plot([max(FPR),1], [max(TPR),1], color='darkorange', lw=lw)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot sensitivity and specificity curve against different priors\n",
    "def plot_sens_spec(perf_list):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    lw = 2\n",
    "    probClass1 = []\n",
    "    x = 0\n",
    "    while x<1:\n",
    "        x+=.005\n",
    "        probClass1.append(x)\n",
    "\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    for i in range(len(perf_list)):\n",
    "        x = perf_list[i][0]\n",
    "        sensitivity.append(x)\n",
    "        y = perf_list[i][1]\n",
    "        specificity.append(y)\n",
    "        \n",
    "    line1 = plt.plot(probClass1, sensitivity,'darkorange',lw = lw,label ='sensitivity')\n",
    "    line2 = plt.plot(probClass1, specificity,'b',lw = lw,label ='specificity')\n",
    "\n",
    "    plt.xlabel('Prior Probabilities')\n",
    "#     plt.ylabel('Accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title('Sensitivity & Specificity Curve for Different Prior Probabilities (KNN)')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy for diffrent K\n",
    "def plot_k(trainingSet,testSet,prior0):\n",
    "    K, acc = vary_k (trainingSet,testSet,prior0)   \n",
    "    # plot accuracy with respect to k\n",
    "    plt.figure(figsize=(6,5))\n",
    "    line = plt.plot(K, acc,'ro-',lw = 2)\n",
    "    plt.xlabel('k values')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Performance Curve for Different k values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sensitivity and specificity curve against different eigenvalues in PCA\n",
    "def vary_eigVal(trainfile,testfile):\n",
    "    trainingSet, testSet = loadDataset (trainfile,testfile)\n",
    "    pri,predictions = KNN(trainingSet,testSet,11,132/200)\n",
    "    perform = performance_measure(testSet, predictions)\n",
    "    perf = performance(perform)  \n",
    "    return perf\n",
    "\n",
    "p1 = vary_eigVal('pX_PIMA_TR_0.01.txt','pX_PIMA_TE_0.01.txt')\n",
    "p2 = vary_eigVal('pX_PIMA_TR_0.05.txt','pX_PIMA_TE_0.05.txt')\n",
    "p3 = vary_eigVal('pX_PIMA_TR_0.1.txt','pX_PIMA_TE_0.1.txt')\n",
    "p4 = vary_eigVal('pX_PIMA_TR_0.2.txt','pX_PIMA_TE_0.2.txt')\n",
    "p5 = vary_eigVal('pX_PIMA_TR_0.4.txt','pX_PIMA_TE_0.4.txt')\n",
    "p6 = vary_eigVal('pX_PIMA_TR_0.5.txt','pX_PIMA_TE_0.5.txt')\n",
    "p7 = vary_eigVal('pX_PIMA_TR_0.7.txt','pX_PIMA_TE_0.7.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot sensitivity and specificity curve against different eigenvalues in PCA\n",
    "def plot_vary_eigVal(p1,p2,p3,p4,p5,p6,p7):\n",
    "    e = [x for x in range(1,8)]\n",
    "    sensitivity = [p7[0],p6[0],p5[0],p4[0],p3[0],p2[0],p1[0]]\n",
    "    specificity = [p7[1],p6[1],p5[1],p4[1],p3[1],p2[1],p1[1]]\n",
    "    \n",
    "    line1 = plt.plot(e, sensitivity,'darkorange',lw = 2,label ='sensitivity')\n",
    "    line2 = plt.plot(e, specificity,'b',lw = 2,label ='specificity')\n",
    "\n",
    "    plt.xlabel('Number of Eigenvalues')\n",
    "#     plt.ylabel('Accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title('Sensitivity & Specificity Curve for Different Eigenvalues (KNN)')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold validation on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold validation on the training set\n",
    "#split the training set into train and test \n",
    "def kfold_split(n_split, x_len):\n",
    "    index = []\n",
    "    test_size = int(x_len/n_split)\n",
    "    train_size = x_len - test_size\n",
    "    for i in range(n_split):\n",
    "        j = i*test_size\n",
    "        index.append([list(set(list(range(0,x_len))).difference(set(list(range(j,j+test_size)))) ),\n",
    "                     list(range(j,j+test_size))])\n",
    "    return index\n",
    "\n",
    "index = kfold_split(10,200)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean accuracy of all K-folds for diffrent k in KNN\n",
    "def cross_validation_score(trainingSet, n_split,prior0):\n",
    "    train = pd.DataFrame(trainingSet)\n",
    "    col = len (train.columns)\n",
    "    X = train.iloc[:,0:col].values\n",
    "    index = kfold_split(n_split, len(X))\n",
    "    acc = []\n",
    "    mean_k = []\n",
    "    \n",
    "    for train_index, test_index in index:\n",
    "        x = X[train_index]\n",
    "        y = X[test_index]\n",
    "        K, ac = vary_k(x,y,prior0)\n",
    "        acc.append(ac) \n",
    "    for k in range(len(K)):\n",
    "        avg = (acc[0][k]+acc[1][k]+acc[2][k]+acc[3][k]+acc[4][k])/5\n",
    "        mean_k.append(avg)\n",
    "\n",
    "    return mean_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot validation accuracy with classification accuracy\n",
    "def validation_plot(trainingSet,testSet,n_split,prior0):\n",
    "    K, acc = vary_k (trainingSet,testSet,prior0)   \n",
    "    true_rate = cross_validation_score(trainingSet, n_split,prior0)\n",
    "    # plot accuracy with respect to k\n",
    "    plt.figure(figsize=(6,5))\n",
    "    line1 = plt.plot(K, acc,'ro-',lw = 2, label = 'classification accuracy')\n",
    "    line2 = plt.plot(K, true_rate,'bo-',lw = 2, label = 'validation accuracy')\n",
    "    plt.xlabel('k values')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend (loc = 'lower right')\n",
    "    plt.title('Performance Curve for Different k values')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "####check with built in classifier of sklearn for model accuracy\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.model_selection import KFold # import KFold\n",
    "# k_range=range(1,15)\n",
    "# #define a list\n",
    "# k_results=[]\n",
    "# train = pd.DataFrame(trainingSet)\n",
    "# col = len (train.columns)\n",
    "# X = train.iloc[:,0:col-1].values\n",
    "# y = train.iloc[:,col-1].values\n",
    "\n",
    "# for k in k_range:\n",
    "#     knn=KNeighborsClassifier(n_neighbors=k)\n",
    "#     #K-Fold Cross Valdation\n",
    "#     from sklearn.model_selection import cross_val_score\n",
    "#     #define an object named 'results' tostore the accuracy scores\n",
    "#     #no. of iterations=5\n",
    "#     results=cross_val_score(knn,X,y,cv=5,scoring='accuracy')\n",
    "#     #print(results)\n",
    "#     #store the mean value kor k=1 to 15 ina list\n",
    "#     k_results.append(results.mean())\n",
    "# #print all mean values for k=1 to 15 \n",
    "# print(k_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output cells\n",
    "\n",
    "uncomment for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_vary_eigVal(p1,p2,p3,p4,p5,p6,p7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perf = perf_list (trainingSet,testSet,15)\n",
    "# plot_roc (perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perfp = perf_list (trainingSetp,testSetp,11)\n",
    "# plot_roc (perfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perff = perf_list (trainingSetf,testSetf,7)\n",
    "# plot_roc (perff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_sens_spec(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_sens_spec(perfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_sens_spec(perff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_plot(trainingSet,testSet,10,132/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_plot(trainingSetp,testSetp,10,132/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_plot(trainingSetf,testSetf,10,132/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
